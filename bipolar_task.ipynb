{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "bipolar_task.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TlcsuUnQfPlf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C6qhjmw7fPlq"
      },
      "outputs": [],
      "source": [
        "def get_headlines():\n",
        "    \n",
        "    def extract_hindustantimes(WebUrl,lis, value): #lis is a dict containing all the headings\n",
        "        url = WebUrl\n",
        "        code = requests.get(url)\n",
        "        plain = code.text\n",
        "        s = BeautifulSoup(plain, \"html.parser\")\n",
        "        for link in s.findAll('div', {'class':'media-heading headingfour'}):\n",
        "            for link2 in link.findAll('a'):\n",
        "                title = link2.get('title')\n",
        "                lis[title] = value\n",
        "    # Getting all the viral headlines\n",
        "    viral = {}\n",
        "    extract_hindustantimes('https://www.hindustantimes.com/it-s-viral/',viral,1)\n",
        "    for i in range(2,15):\n",
        "        url = 'https://www.hindustantimes.com/it-s-viral/?pageno=' + str(i)\n",
        "        extract_hindustantimes(url,viral,1)\n",
        "    print('Number of viral news headlines: ',len(viral))\n",
        "\n",
        "    # Getting all the non-viral headlines\n",
        "    non_viral = {}\n",
        "    extract_hindustantimes('https://www.hindustantimes.com/latest-news/',non_viral,0)\n",
        "    for i in range(2,55):\n",
        "        url = 'https://www.hindustantimes.com/latest-news/?pageno=' + str(i)\n",
        "        extract_hindustantimes(url,non_viral,0)\n",
        "\n",
        "    # Remove the viral news headlines from the non_viral dictionary\n",
        "    del_keys = []\n",
        "    for key in non_viral:\n",
        "        if key in set(viral.keys()): # Using a set speeds up the process considerably\n",
        "            del_keys.append(key)\n",
        "    for key in del_keys:\n",
        "        del non_viral[key]\n",
        "    print('Number of non-viral news headlines: ',len(non_viral))\n",
        "    \n",
        "    return viral,non_viral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "AaYL0IMzHiJt",
        "outputId": "efc26237-b636-43e5-f8d5-51c933e52ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of viral news headlines:  221\n",
            "Number of non-viral news headlines:  750\n"
          ]
        }
      ],
      "source": [
        "viral, non_viral = get_headlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8bQ00nKifPl8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame(viral.items(), columns = ['headlines','label'])\n",
        "data = data.append(pd.DataFrame(non_viral.items(), columns = ['headlines','label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "colab_type": "code",
        "id": "YeapF6cUfPmD",
        "outputId": "47c48b1e-24a3-474e-e66c-dce77c62eaf8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wuhan’s L-strain may be behind Gujarat’s high ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It would be ‘golden opportunity’ to learn from...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Employees of private firms in Gurugram may hav...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Education department braces for challenges in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Plane lands in the middle of busy highway, peo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>‘Need to strengthen economic activities, comba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Will India Couture Week 2020 be a reality?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>We needed a break, but not in this way: Rinku ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>‘You and I will meet again’: Mumbai Police use...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Sofosh appeals for donations amid lockdown sca...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headlines  label\n",
              "0   Wuhan’s L-strain may be behind Gujarat’s high ...      0\n",
              "1   It would be ‘golden opportunity’ to learn from...      0\n",
              "2   Employees of private firms in Gurugram may hav...      0\n",
              "3   Education department braces for challenges in ...      0\n",
              "4   Plane lands in the middle of busy highway, peo...      1\n",
              "..                                                ...    ...\n",
              "95  ‘Need to strengthen economic activities, comba...      0\n",
              "96        Will India Couture Week 2020 be a reality?       0\n",
              "97  We needed a break, but not in this way: Rinku ...      0\n",
              "98  ‘You and I will meet again’: Mumbai Police use...      1\n",
              "99  Sofosh appeals for donations amid lockdown sca...      0\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Shuffling the dataset\n",
        "data = data.sample(frac=1,  random_state=10).reset_index(drop=True)\n",
        "train = data[:-100]\n",
        "train.to_csv('train.csv', index = False)\n",
        "test = data[-100:].reset_index(drop = True)\n",
        "train.to_csv('test.csv', index = False)\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "colab_type": "code",
        "id": "5YYe8AmjfPmK",
        "outputId": "0372b4de-45b8-4e93-8abd-8a0bdf3e672b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "import string\n",
        "import nltk.classify.util\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_wds = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qHDGYbHYfPmP"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(corpus):\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_freq = {}\n",
        "    sentences_tokenized = []\n",
        "\n",
        "    for sentence in corpus:\n",
        "        # Removing all the punctuations in the text\n",
        "        sentence2 = sentence.translate({ord(c):'' for c in string.punctuation + \"‘’\"})\n",
        "        words = wordpunct_tokenize(sentence2.lower())\n",
        "        # Grouped similar words together to decrease noise in the data\n",
        "        for i in range(len(words)):\n",
        "            words[i] = lemmatizer.lemmatize(words[i])\n",
        "            if words[i] not in set(word_freq.keys()):\n",
        "                word_freq[words[i]] = 1\n",
        "            else:\n",
        "                word_freq[words[i]] += 1\n",
        "                \n",
        "        sentences_tokenized.append(words)\n",
        "    # word_freq contains the frequency of each word\n",
        "    # I have filtered out the words which occur only once so that noise is reduced\n",
        "    word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    most_freq = {}\n",
        "    for key in word_freq:\n",
        "        if word_freq[key]>1:\n",
        "            most_freq[key] = word_freq[key]\n",
        "            \n",
        "\n",
        "    processed_sent = []\n",
        "    for sent in sentences_tokenized:\n",
        "        useful_words = [word for word in sent if word in most_freq]\n",
        "        processed_sent.append(\" \".join(useful_words))\n",
        "    return processed_sent\n",
        "        \n",
        "train_processed_sentences = preprocess_text(train['headlines'])\n",
        "test_processed_sentences = preprocess_text(test['headlines'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I have used the TF-IDF Vectorizer to vectorize the words in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RF3Ft2uQfPmT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer()\n",
        "train_final = vect.fit_transform(train_processed_sentences)\n",
        "test_final = vect.transform(test_processed_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lI9OAWzYfPmc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_final, train['label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "colab_type": "code",
        "id": "pxwLmli4fPml",
        "outputId": "962ba6a3-5248-46dd-d124-0f0a0424e95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.23)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CatBoost model was used since it gave the best performance with a Validation Accuracy = 87.4% and F1 Score = 0.725"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "MQFUrsyMfPmg",
        "outputId": "e86da798-b458-4f5f-c070-7ba16c2b404f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  0.8742857142857143\n",
            "F1 score :  0.7250000000000001\n"
          ]
        }
      ],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = CatBoostClassifier(eval_metric='F1', iterations=1000, max_depth=6, learning_rate=0.03)\n",
        "# model = LGBMClassifier(n_estimators=1100, eval_metric = 'F1')\n",
        "model.fit(X_train, y_train, eval_set=[(X_val,y_val)], verbose=0)\n",
        "y_pred = model.predict(X_val)\n",
        "print('Validation Accuracy : ',  model.score(X_val,y_val))\n",
        "print('F1 score : ', f1_score(y_val,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bDqooQA9gSWS"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict_proba(test_final)\n",
        "result = pd.DataFrame()\n",
        "result['headlines'] = test['headlines']\n",
        "result['virality probability'] = predictions[:,1]*100\n",
        "result['actual virality'] = ['Yes' if i==1 else \"No\" for i in test['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "bfaNtkbtvDD0",
        "outputId": "0efdceb8-bd85-4962-8a85-e7c781b88ca9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>virality probability</th>\n",
              "      <th>actual virality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wuhan’s L-strain may be behind Gujarat’s high ...</td>\n",
              "      <td>3.252530</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It would be ‘golden opportunity’ to learn from...</td>\n",
              "      <td>10.054348</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Employees of private firms in Gurugram may hav...</td>\n",
              "      <td>1.946367</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Education department braces for challenges in ...</td>\n",
              "      <td>16.057338</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Plane lands in the middle of busy highway, peo...</td>\n",
              "      <td>85.690024</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines  ...  actual virality\n",
              "0  Wuhan’s L-strain may be behind Gujarat’s high ...  ...               No\n",
              "1  It would be ‘golden opportunity’ to learn from...  ...               No\n",
              "2  Employees of private firms in Gurugram may hav...  ...               No\n",
              "3  Education department braces for challenges in ...  ...               No\n",
              "4  Plane lands in the middle of busy highway, peo...  ...              Yes\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uGBLpTyKJhg1"
      },
      "outputs": [],
      "source": [
        "result.to_csv('test_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GHYt6vUvKbha"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}